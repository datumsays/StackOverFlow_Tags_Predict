{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modules Requires\n",
    "\n",
    "# Common\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Special tools\n",
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "\n",
    "# Sklearn \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch data\n",
    "os.chdir('/Users/daniellee/Desktop/Kaggle/data/stackoverflow_data/part1_approach1_output/')\n",
    "testX, trainX = map(pd.read_csv, glob.glob('*.csv')[:2])\n",
    "testy = pd.read_csv('testy.csv')\n",
    "trainy = pd.read_csv('trainy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Dictionary\n",
    "os.chdir('/Users/daniellee/Desktop/Kaggle/data/stackoverflow_data/')\n",
    "tfidf_selection = np.load('top_k_features.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) K Topic Chooser Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_number_chooser(target_series, k_per_class):\n",
    "    \"\"\" Choose k features per topic based on inverse quanitity\n",
    "        of document class count and number of k features selected\n",
    "        per class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of vectors\n",
    "    _len = len(target_series)\n",
    "    unique_nums = len(target_series.unique())\n",
    "    total_features = k_per_class * unique_nums\n",
    "    \n",
    "    # Get a pandas groupby with proportions of k features\n",
    "    frac = target_series.value_counts().map(lambda x: round(1 - float(x)/_len, 2))\n",
    "    frac_sum = sum(frac)\n",
    "    new_prop = frac.map(lambda x: round(x/frac_sum * total_features))\n",
    "    \n",
    "    # If the sum of the k features and the series length \n",
    "    # unequal that randomly add one to one of the class feature\n",
    "    if sum(new_prop) > total_features:\n",
    "        plusone = np.random.choice(unique_nums)\n",
    "        new_prop[plusone] = new_prop[plusone] - (sum(new_prop) - total_features)\n",
    "        \n",
    "    elif sum(new_prop) < total_features:\n",
    "        plusone = np.random.choice(unique_nums)\n",
    "        new_prop[plusone] = new_prop[plusone] + (sum(new_prop) - total_features)\n",
    "\n",
    "    return new_prop.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_selection['diy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GetBowDummies_Array2(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs: (1) Series with a text vector (2) Bag of Words for features\n",
    "    Output: Dataframe with dummy variables indicating whether a feature word is present in a row.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    train = pd.Series(['I dont know','polish dont','fire','healthcare know','healthcare'])\n",
    "    test  = pd.Series(['I dont know','healthcare know'])\n",
    "    feats = ['dont','know','healthcare']\n",
    "\n",
    "    train_bow_dummies = GetBowDummies(train, feats).get_bow_dummies()\n",
    "\n",
    "    test_bow_dummies = GetBowDummies(test, feats).get_bow_dummies()\n",
    "\n",
    "    test_bow_dummies\n",
    "     >> dont  know  healthcare\n",
    "        0     0     0           0\n",
    "        1     0     0           0\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    def __init__(self, series, features):\n",
    "        \"\"\"\n",
    "        :param series: A column containing raw text\n",
    "        :param features: A list of feature words\n",
    "        \"\"\"\n",
    "        features.sort()\n",
    "        \n",
    "        self.series = series\n",
    "        self.index  = self.series.index\n",
    "        self.features = np.asarray(features)\n",
    "\n",
    "        # Define dimension\n",
    "        self.nrows = series.shape[0]\n",
    "        self.ncols = len(features)\n",
    "        self.dim   = (self.nrows, self.ncols)\n",
    "\n",
    "    def index_feats_dict(self):\n",
    "        \"\"\"\n",
    "        For every document row, features present in doc\n",
    "        identified.\n",
    "        \"\"\"\n",
    "        # doc_features_list = []\n",
    "        zero_matrix = np.zeros(self.dim, np.int)\n",
    "\n",
    "        for i, doc in enumerate(self.series):\n",
    "            # Sets for a doc and feature words\n",
    "            \n",
    "            doc_set = set(doc.split())\n",
    "            feat_set = set(self.features)\n",
    "\n",
    "            # Shared words between the two sets\n",
    "            interset_words = np.asarray(list(doc_set.intersection(feat_set)))\n",
    "            \n",
    "            if len(interset_words) != 0: \n",
    "                ndx = np.searchsorted(self.features, interset_words)\n",
    "                zero_matrix[i,ndx] = 1\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        return zero_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 completed\n",
      "100 completed\n",
      "500 completed\n",
      "1000 completed\n",
      "3000 completed\n",
      "5000 completed\n",
      "7500 completed\n"
     ]
    }
   ],
   "source": [
    "## Perform Feature Selection using top K TF-IDF per Class\n",
    "\n",
    "# Choose kth threshold\n",
    "\n",
    "k_to_test = [50, 100, 500, 1000, 3000, 5000, 7500, 10000]\n",
    "model_result_dict = {}\n",
    "k_model_dict = {}\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encode = le.fit(trainy['category'])\n",
    "train_true_y = le.transform(trainy['category'])\n",
    "test_true_y = le.transform(testy['category'])\n",
    "\n",
    "for k in k_to_test:\n",
    "    \n",
    "    # Fetch k features to select\n",
    "    k_value_counts = k_number_chooser(trainy['category'], k)\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    topic_k_groupby = k_number_chooser(trainy['category'], k)\n",
    "    \n",
    "    top_k_features_per_topic = {}\n",
    "    for topic in trainy['category'].unique():\n",
    "        feats_k_count = topic_k_groupby[topic]\n",
    "        top_k_features_per_topic[topic] = tfidf_selection[topic][:feats_k_count]\n",
    "\n",
    "    # Create Bow dummies\n",
    "    feats = list(chain.from_iterable(top_k_features_per_topic.values()))\n",
    "    train_bow = GetBowDummies_Array2(trainX['combined'], feats).index_feats_dict()\n",
    "    test_bow = GetBowDummies_Array2(testX['combined'], feats).index_feats_dict()\n",
    "    \n",
    "    # Fit Model \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X=train_bow, y=trainy['category'])\n",
    "    k_model_dict[k] = mnb\n",
    "    \n",
    "    # Make train and test predictions\n",
    "    train_pred_y = mnb.predict(train_bow)\n",
    "    test_pred_y = mnb.predict(test_bow)\n",
    "    \n",
    "    # Measure time\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Apply encoder \n",
    "    train_pred_y = le.transform(train_pred_y)\n",
    "    test_pred_y = le.transform(test_pred_y)\n",
    "    \n",
    "    # Evaluate F1\n",
    "    trainF1 = f1_score(train_pred_y, train_true_y, average='macro') \n",
    "    testF1 = f1_score(test_pred_y, test_true_y, average='macro')\n",
    "    \n",
    "    # Store the result in dictionary \n",
    "    model_result = {'duration': duration, 'trainF1': trainF1, 'testF1': testF1, \\\n",
    "                    'train_pred_y': train_pred_y, 'test_pred_y': test_pred_y}\n",
    "\n",
    "    model_result_dict[k] = model_result\n",
    "    \n",
    "    print(k, 'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_result_dict"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
