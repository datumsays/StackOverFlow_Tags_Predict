{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>category</th>\n",
       "      <th>combined</th>\n",
       "      <th>content_size</th>\n",
       "      <th>title_size</th>\n",
       "      <th>tags_size</th>\n",
       "      <th>TitleNTags</th>\n",
       "      <th>ContentNTags</th>\n",
       "      <th>title_pred</th>\n",
       "      <th>content_pred</th>\n",
       "      <th>combined_pred</th>\n",
       "      <th>title_nouns</th>\n",
       "      <th>content_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>criticality ribosome binding site relative sta...</td>\n",
       "      <td>prokaryotic translation critical efficient tra...</td>\n",
       "      <td>['ribosome', 'binding-sites', 'translation', '...</td>\n",
       "      <td>biology</td>\n",
       "      <td>criticality ribosome binding site relative sta...</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>criticality ribosome prokaryotic codon</td>\n",
       "      <td>translation 7b observable prokaryotic</td>\n",
       "      <td>translation prokaryotic ribosome codon</td>\n",
       "      <td>criticality ribosome binding site relative sta...</td>\n",
       "      <td>prokaryotic translation critical efficient tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>rnase contamination rna based experiments prev...</td>\n",
       "      <td>anyone suggestions prevent rnase contamination...</td>\n",
       "      <td>['rna', 'biochemistry']</td>\n",
       "      <td>biology</td>\n",
       "      <td>rnase contamination rna based experiments prev...</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>prevented rnase contamination experiments</td>\n",
       "      <td>rnase depc pipette degradation</td>\n",
       "      <td>rnase contamination rna depc</td>\n",
       "      <td>rnase contamination rna based experiment preve...</td>\n",
       "      <td>doe anyone suggestion prevent rnase contaminat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>lymphocyte sizes clustered two groups</td>\n",
       "      <td>tortora writes principles anatomy physiology l...</td>\n",
       "      <td>['immunology', 'cell-biology', 'hematology']</td>\n",
       "      <td>biology</td>\n",
       "      <td>lymphocyte sizes clustered two groups tortora ...</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>clustered lymphocyte groups sizes</td>\n",
       "      <td>lymphocytes diameter tortora 14</td>\n",
       "      <td>lymphocytes clustered groups sizes</td>\n",
       "      <td>lymphocyte size clustered group</td>\n",
       "      <td>tortora principle anatomy physiology lymphocyt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1  id  \\\n",
       "0           0             0             0   1   \n",
       "1           1             1             1   2   \n",
       "2           2             2             2   3   \n",
       "\n",
       "                                               title  \\\n",
       "0  criticality ribosome binding site relative sta...   \n",
       "1  rnase contamination rna based experiments prev...   \n",
       "2              lymphocyte sizes clustered two groups   \n",
       "\n",
       "                                             content  \\\n",
       "0  prokaryotic translation critical efficient tra...   \n",
       "1  anyone suggestions prevent rnase contamination...   \n",
       "2  tortora writes principles anatomy physiology l...   \n",
       "\n",
       "                                                tags category  \\\n",
       "0  ['ribosome', 'binding-sites', 'translation', '...  biology   \n",
       "1                            ['rna', 'biochemistry']  biology   \n",
       "2       ['immunology', 'cell-biology', 'hematology']  biology   \n",
       "\n",
       "                                            combined  content_size  \\\n",
       "0  criticality ribosome binding site relative sta...            24   \n",
       "1  rnase contamination rna based experiments prev...            21   \n",
       "2  lymphocyte sizes clustered two groups tortora ...            31   \n",
       "\n",
       "   title_size  tags_size  TitleNTags  ContentNTags  \\\n",
       "0           9          4           2             2   \n",
       "1           6          2           1             1   \n",
       "2           5          3           0             0   \n",
       "\n",
       "                                  title_pred  \\\n",
       "0     criticality ribosome prokaryotic codon   \n",
       "1  prevented rnase contamination experiments   \n",
       "2          clustered lymphocyte groups sizes   \n",
       "\n",
       "                            content_pred  \\\n",
       "0  translation 7b observable prokaryotic   \n",
       "1         rnase depc pipette degradation   \n",
       "2        lymphocytes diameter tortora 14   \n",
       "\n",
       "                            combined_pred  \\\n",
       "0  translation prokaryotic ribosome codon   \n",
       "1            rnase contamination rna depc   \n",
       "2      lymphocytes clustered groups sizes   \n",
       "\n",
       "                                         title_nouns  \\\n",
       "0  criticality ribosome binding site relative sta...   \n",
       "1  rnase contamination rna based experiment preve...   \n",
       "2                    lymphocyte size clustered group   \n",
       "\n",
       "                                       content_nouns  \n",
       "0  prokaryotic translation critical efficient tra...  \n",
       "1  doe anyone suggestion prevent rnase contaminat...  \n",
       "2  tortora principle anatomy physiology lymphocyt...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob \n",
    "\n",
    "os.chdir('/Users/daniellee/Desktop/Kaggle/data/stackoverflow_data/')\n",
    "\n",
    "train = pd.read_csv('cleaned/topic_model_df_train.csv'); train.head(3)\n",
    "test = pd.read_csv('cleaned/topic_model_df_test.csv'); train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Partitions of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "partitions = train_test_split(train.drop('category',axis=1), train['category'])\n",
    "\n",
    "os.chdir('/Users/daniellee/Desktop/Kaggle/data/stackoverflow_data/experiment_test_set/')\n",
    "for name, part in zip(['trainX', 'testX', 'trainy', 'testy'], partitions):\n",
    "    part.to_csv(name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy = [df.reset_index() for df in partitions] \n",
    "trainX = trainX.ix[:,5:]\n",
    "testX = testX.ix[:,5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Text Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove blanks\n",
    "trainX['content'] = trainX['content'].fillna('')\n",
    "testX['content'] = testX['content'].fillna('')\n",
    "\n",
    "trainBlankLoc = trainX['content'] == ''\n",
    "testBlankLoc = testX['content'] == ''\n",
    " \n",
    "trainX.ix[trainBlankLoc, 'combined'] = trainX.ix[trainBlankLoc, 'title'] + trainX.ix[trainBlankLoc, 'content']\n",
    "testX.ix[testBlankLoc, 'combined'] = testX.ix[testBlankLoc, 'title'] + testX.ix[testBlankLoc, 'content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX['stemmed'] = ''\n",
    "testX['stemmed'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "def stemmer(x):\n",
    "    return ' '.join([snow.stem(word) for word in x.split()]) \n",
    "\n",
    "trainX['combined'] = trainX['combined'].map(stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_digit(text):\n",
    "    try:\n",
    "        float(text)\n",
    "        return True \n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def numeric_replacer(x):\n",
    "    new_set = []\n",
    "    for word in x.split():\n",
    "        if is_digit(word):\n",
    "            new_set.append('digitstring')\n",
    "        else:\n",
    "            new_set.append(word)\n",
    "    \n",
    "    return ' '.join(new_set)\n",
    "\n",
    "trainX['combined'] = trainX['combined'].map(numeric_replacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GetBowDummies(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs: (1) Series with a text vector (2) Bag of Words for features\n",
    "    Output: Dataframe with dummy variables indicating whether a feature word is present in a row.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    train = pd.Series(['I dont know','polish dont','fire','healthcare know','healthcare'])\n",
    "    test  = pd.Series(['I dont know','healthcare know'])\n",
    "    feats = ['dont','know','healthcare']\n",
    "\n",
    "    train_bow_dummies = GetBowDummies(train, feats).get_bow_dummies()\n",
    "\n",
    "    test_bow_dummies = GetBowDummies(test, feats).get_bow_dummies()\n",
    "\n",
    "    test_bow_dummies\n",
    "     >> dont  know  healthcare\n",
    "        0     0     0           0\n",
    "        1     0     0           0\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    def __init__(self, series, features):\n",
    "        \"\"\"\n",
    "        :param series: A column containing raw text\n",
    "        :param features: A list of feature words\n",
    "        \"\"\"\n",
    "        self.series = series\n",
    "        self.index  = self.series.index\n",
    "        self.features = features\n",
    "\n",
    "        # Define dimension\n",
    "        self.nrows = series.shape[0]\n",
    "        self.ncols = len(features)\n",
    "        self.dim   = (self.nrows, self.ncols)\n",
    "\n",
    "    def index_feats_dict(self):\n",
    "        \"\"\"\n",
    "        For every document row, features present in doc\n",
    "        identified.\n",
    "        \"\"\"\n",
    "        doc_features_dict = {}\n",
    "\n",
    "        for index, doc in zip(self.index, self.series):\n",
    "            # Sets for a doc and feature words\n",
    "            \n",
    "            doc_set = set(doc.split())\n",
    "            feat_set = set(self.features)\n",
    "\n",
    "            # Shared words between the two sets\n",
    "            interset_words = doc_set.intersection(feat_set)\n",
    "\n",
    "            # Append to doc_features_dict\n",
    "            doc_features_dict[index] = list(interset_words)\n",
    "\n",
    "        return doc_features_dict\n",
    "\n",
    "    def get_bow_dummies(self):\n",
    "        \"\"\"\n",
    "        Replace 0's with 1 in positions of a bow dataframe\n",
    "        to indicate that feature words are present in docs\n",
    "        \"\"\"\n",
    "\n",
    "        # Get an np matrix of zeros based on defined dim\n",
    "        zero_matrix = np.zeros(self.dim, np.int)\n",
    "\n",
    "        # Create a dataframe containing feature columns and 0's\n",
    "        self.zero_df = pd.DataFrame(zero_matrix, columns=self.features)\n",
    "\n",
    "        # Get a dictionary of index and features per doc\n",
    "        doc_features_dict = self.index_feats_dict()\n",
    "        doc_ids = doc_features_dict.keys()\n",
    "        doc_feats = doc_features_dict.values()\n",
    "\n",
    "        # For each row in zero_df, indicate 1 for every\n",
    "        # feature word present in a doc of a dataframe\n",
    "        for index, feats in zip(doc_ids, doc_feats):\n",
    "            self.zero_df.ix[index, feats] = 1\n",
    "\n",
    "        return self.zero_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "def topic_index(series, topic):\n",
    "    return series.ix[series == topic].index\n",
    "\n",
    "def text_joiner(x):\n",
    "    return ' '.join(x)\n",
    "\n",
    "def corpusCreator(df, labelSeries):\n",
    "    \n",
    "    text = {}\n",
    "    for topic in labelSeries.unique():\n",
    "        index = topic_index(labelSeries, topic)\n",
    "        topicText = []\n",
    "        for sent in df[index]:\n",
    "            topicText += sent.split()\n",
    "        text[topic] = topicText \n",
    "    \n",
    "    return text\n",
    "    \n",
    "def tfidf(bags_of_words):\n",
    "    \n",
    "    \"\"\" Fetches the top words in k based on TF-IDF scores.\n",
    "        Returns a list of words with top K.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fetch id for each word\n",
    "    idDict = corpora.Dictionary(bags_of_words)\n",
    "    \n",
    "    # Get the reverse key-vaule mapping\n",
    "    inv_Dict = {v:k for v, k in idDict.items()}\n",
    "    \n",
    "    # Transform tCorpus into vector form\n",
    "    vCorpus = [idDict.doc2bow(tokens) for tokens in bags_of_words]\n",
    "\n",
    "    # Fit TFIDF\n",
    "    tfidf = models.TfidfModel(vCorpus)\n",
    "    \n",
    "    return tfidf, inv_Dict, vCorpus\n",
    "\n",
    "def tfidf_at_k(tfidf, doc_vector, word_key, k):\n",
    "    \n",
    "    top_k_id_scores = sorted(tfidf[doc_vector], key=itemgetter(1), reverse=True)[:k]\n",
    "    \n",
    "    return [word_key[key] for key, word in top_k_id_scores] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "## Perform Feature Selection using top K TF-IDF per Class\n",
    "\n",
    "# Choose kth threshold\n",
    "\n",
    "class_dictionary = corpusCreator(trainX['combined'], trainy['category'])\n",
    "topics = class_dictionary.keys()\n",
    "mod, word_key, vCorpus = tfidf(list(class_dictionary.values()))\n",
    "topic_vector = {topic: document for topic, document in zip(topics, vCorpus)}\n",
    "\n",
    "k_to_test = [50, 100, 500, 1000, 5000, 8000, 10000]\n",
    "trainF1 = []\n",
    "testF1  = []\n",
    "k_model_dict = {}\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encode = le.fit(trainy['category'])\n",
    "train_true_y = le.transform(trainy['category'])\n",
    "test_true_y = le.transform(testy['category'])\n",
    "\n",
    "for k in k_to_test:\n",
    "    \n",
    "    top_k_features_per_topic = {}\n",
    "    for topic in topics:\n",
    "        top_k_features = tfidf_at_k(mod, topic_vector[topic], word_key, k)\n",
    "        top_k_features_per_topic[topic] = top_k_features\n",
    "\n",
    "    # Create Bow dummies\n",
    "    feats = list(chain.from_iterable(top_k_features_per_topic.values()))\n",
    "    train_bow = GetBowDummies(trainX['combined'], feats).get_bow_dummies()\n",
    "    test_bow = GetBowDummies(testX['combined'], feats).get_bow_dummies()\n",
    "    \n",
    "    # Fit Model \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X=train_bow, y=trainy['category'])\n",
    "    k_model_dict[k] = mnb\n",
    "    joblib.dump(mnb, str(k)+'_'+'mnb_model.pkl') \n",
    "    \n",
    "    # Make train and test predictions\n",
    "    train_pred_y = mnb.predict(train_bow)\n",
    "    test_pred_y = mnb.predict(test_bow)\n",
    "    \n",
    "    # Apply encoder \n",
    "    train_pred_y = le.transform(train_pred_y)\n",
    "    test_pred_y = le.transform(test_pred_y)\n",
    "    \n",
    "    # Evaluate F1\n",
    "    trainF1.append(f1_score(train_pred_y, train_true_y, average='macro'))\n",
    "    testF1.append(f1_score(test_pred_y, test_true_y, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Columns for Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testF1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
